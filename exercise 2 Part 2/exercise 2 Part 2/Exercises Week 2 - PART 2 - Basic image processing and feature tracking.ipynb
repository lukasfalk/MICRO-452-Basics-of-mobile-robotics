{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T20:09:36.824740Z",
     "start_time": "2020-05-04T20:09:36.816919Z"
    }
   },
   "source": [
    "<div style=\"border:1px solid black; padding:20px 20px;text-align: justify;text-justify: inter-word\">\n",
    "    <strong>Exercise Session 2 - *PART 2* (over 2) - Computer vision: basics of computer vision. The first part is done in the interactive session <br/> </strong><br/><br/>\n",
    "    <span style=\"text-decoration:underline;font-weight:bold;\">How to use this notebook?</span><br/>\n",
    "    This notebook is made of text cells and code cells. The code cells have to be <strong>executed</strong> to see the result of the program. To execute a cell, simply select it and click on the \"play\" button (<span style=\"font: bold 12px/30px Arial, serif;\">&#9658;</span>) in the tool bar just above the notebook, or type <code>shift + enter</code>. It is important to execute the code cells in their order of appearance in the notebook.<br/>\n",
    "You can make use of the table of contents to navigate easily between sections.\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"justify;text-justify: inter-word\">\n",
    "So that you may familiarise with the notebooks and the basic python syntax, the exercises are provided in notebook form and whenever there are any calculations to be made, we encourage you to do them by code. Also, if you want to take notes, we encourage you to use the markdown or Raw NBConvert cells. \n",
    "</div>\n",
    "\n",
    "Note : the images used in this exercise session were taken from tutorials online. And although the references are not provided here so that you don't have a sneak peak at the solution, you will not be excused if you do not cite your sources in the project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Learning-Goals\" data-toc-modified-id=\"Learning-Goals-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Learning Goals</a></span></li><li><span><a href=\"#Activity-1---Exploring-Different-Image-Processing-Filters\" data-toc-modified-id=\"Activity-1---Exploring-Different-Image-Processing-Filters-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Activity 1 - Exploring Different Image Processing Filters</a></span><ul class=\"toc-item\"><li><span><a href=\"#Denoising-Filters\" data-toc-modified-id=\"Denoising-Filters-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Denoising Filters</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importing-OpenCV,-Reading-and-Displaying-an-Image\" data-toc-modified-id=\"Importing-OpenCV,-Reading-and-Displaying-an-Image-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Importing OpenCV, Reading and Displaying an Image</a></span></li><li><span><a href=\"#Testing-the-output-of-the-different-filters-on-the-image-to-understand-how-they-work\" data-toc-modified-id=\"Testing-the-output-of-the-different-filters-on-the-image-to-understand-how-they-work-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Testing the output of the different filters on the image to understand how they work</a></span></li></ul></li><li><span><a href=\"#Edge-Detection-Filters\" data-toc-modified-id=\"Edge-Detection-Filters-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Edge Detection Filters</a></span></li></ul></li><li><span><a href=\"#Activity-2---Image-Tracking\" data-toc-modified-id=\"Activity-2---Image-Tracking-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Activity 2 - Image Tracking</a></span></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 2 Learning Goals\n",
    "\n",
    "\n",
    "- Understanding the main steps of computer vision, from cameraframes to high-level information\n",
    "\n",
    "\n",
    "- Implementing the tracking of an object. \n",
    "\n",
    "\n",
    "\n",
    "Note that you will need opencv for this exercise session\n",
    "\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T13:14:16.519011Z",
     "start_time": "2021-09-23T13:14:14.397804Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python tqdm matplotlib numpy ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "!pip install --upgrade tdmclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 1 - Exploring Different Image Processing Filters\n",
    "\n",
    "The first step of many image processing pipelines includes image filtering. \n",
    "\n",
    "\n",
    "You know that filters are applied through convolution and that this can be done in the Fourier space. However, they can also be applied in the image space by computing a moving weighted average between the kernel corresponding to the given filter and the original image. This is illustrated in the image below (taken from [here](https://towardsdatascience.com/types-of-convolution-kernels-simplified-f040cb307c37)). \n",
    "\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "![Pentagon](images/kernel_conv.png)\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "However, rather than go into the detail of the different implementations of convolutions and the kernels associated to the different filters (which do not get me wrong, you do have to know for the exam), the objective here is to take time to learn about the differences between the different filers by using a computer vision library (opencv) to test them on a dataset of images. \n",
    "\n",
    "\n",
    "The types of filters we are intersted (for the moment at least) are :\n",
    "\n",
    "- **Denoising filters (low pass filters)**, which help reduce the amount of noise in an image and smoothen it out. Examples of denoising filters include average filtering, median filtering, gaussian filtering. \n",
    "\n",
    "- **Edge detection filters (high pass filters)**. In the course you have seen the sobel, robert and prewitt filters as well as the Canny Edge filter. \n",
    "\n",
    "\n",
    "If you are interested in having more details about these filtering approaches you can have a look at the introduciton to image filtering provided [here](https://ai.stanford.edu/~syyeung/cvweb/tutorial1.html).\n",
    "\n",
    "\n",
    "## Denoising Filters\n",
    "\n",
    "The goal here is to undestand the different denoising filters that opencv has to offer and when to use them: \n",
    "\n",
    "- **average filtering** with the cv2.blur function\n",
    "\n",
    "\n",
    "- **median filtering** with the cv2.medianBlur function\n",
    "\n",
    "\n",
    "- **gaussian filtering** with the cv2.GaussianBlur function\n",
    "\n",
    "\n",
    "- **bilateral filtering** with the cv2.bilateralFilter function\n",
    "\n",
    "\n",
    "You can call the help function for each of these functions to access the documentation. In the cell below is an example of how to use the help function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T13:14:16.528796Z",
     "start_time": "2021-09-23T13:14:16.522254Z"
    }
   },
   "outputs": [],
   "source": [
    "help(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing OpenCV, Reading and Displaying an Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start by importing opencv for the image processing and matplotlib for the plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T13:14:20.069909Z",
     "start_time": "2021-09-23T13:14:16.532721Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load one of the images and display it. \n",
    "\n",
    "Note that opencv loads images in BGR mode whilst matplotlib (and most systems in general) work in RGB. That is why when we display the image we need to inverse the order of the last dimension\n",
    "\n",
    "Here we have loaded an image that contains Gaussian noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T13:14:20.438028Z",
     "start_time": "2021-09-23T13:14:20.072328Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'images/download.png'\n",
    "img = cv2.imread(filename, cv2.IMREAD_COLOR)\n",
    "\n",
    "plt.imshow(img[:,:,::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the output of the different filters on the image to understand how they work\n",
    "\n",
    "Apply the filters on the original image and display the output. Play with the parameters to see how they influence the output to get a sense of how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:39:37.436589Z",
     "start_time": "2020-05-05T15:39:35.560702Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What is the particularity of the bilateral filter?***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now apply the filters on the `gaussian_noise` image. \n",
    "\n",
    "![Pentagon](images/gaussian_noise.png)\n",
    "\n",
    "***Based on your observations, which filter is most adapted if you want to remove gaussian type noise?***\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:39:39.487965Z",
     "start_time": "2020-05-05T15:39:37.438485Z"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***Which filter would you use if you wanted to remove salt and pepper type noise like the one in the image below?***\n",
    "    \n",
    "<br/>\n",
    "\n",
    "![Pentagon](images/salt_pepper_noise.png)\n",
    "\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Detection Filters\n",
    "\n",
    "Let's focus on three filters:\n",
    "\n",
    "- Sobel (`cv2.Sobel` function): this filter relies on detecting edges in one direction than in the perpendicular to isolate all edges in the image. As it is very close in the underlying principle to the Robers and Prewitt filters we just focus on this one. \n",
    "\n",
    "- Laplacian (`cv2.Laplacian` function): this is a new filter which you did not see in the course. Intead of looking for the maximum of the derivative of an image as the Sobel filter does, this filter looks for the zero crossing of the second derivative. \n",
    "\n",
    "- Canny (`cv2.Canny` function): a bit more elaborate than the previous two, the gradient of the image is computed but there is an extra processing that looks to validate whether a pixel is actually part of an edge or not.\n",
    "\n",
    "\n",
    "\n",
    "Apply the different edge detection filters on the ``SanFrancisco`` image and try to get the best results you can by tuning the parameters of the different algorithms.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](images/SanFrancisco.jpg)\n",
    "\n",
    "<br>\n",
    "\n",
    "***Generally, what should you do before applying the edge detection to improve the results of the filtering?***\n",
    "\n",
    "\n",
    "In particular, for the Sobel filter try to answer the following questions: \n",
    "\n",
    "***When the derivative is along the horizontal axis, what is the effect on the vertical axis?***\n",
    "\n",
    "\n",
    "***How do you implement a Sobel filter that works in both directions?***\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the image in grayscale to be able to apply the edge detection filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T13:14:20.643718Z",
     "start_time": "2021-09-23T13:14:20.439969Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('images/SanFrancisco.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(img,cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can work on the rest :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:39:42.566291Z",
     "start_time": "2020-05-05T15:39:42.302449Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 2 - Image Tracking\n",
    "\n",
    "You can track the position of an object in an image by using what is called template matching. Let's apply template matching on a video of the Thymio robot following a line and attempt to track the position of the robot throughout the video using the different techniques that opencv provides. \n",
    "\n",
    "First we are going to load the video and store the different frames. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T13:14:22.454140Z",
     "start_time": "2021-09-23T13:14:20.645549Z"
    }
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('images/thymio_line_following.mp4')\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if not cap.isOpened(): \n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "video_imgs = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        video_imgs.append(frame)\n",
    "    # Break the loop\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "print(\"There are {} frames in the video\".format(len(video_imgs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take one of the frames and crop the Thymio to make our template. In frame 420, the Thymio is located between x = \\[446, 611\\] and y = \\[312, 484\\] (determined using the cursor on the image). Why are we doing it this way? Template matching does not work well if you take a reference / template image that is not to scale w.r.t the image you are going to be looking in. One solution is to use multi-scale [template matching](https://www.pyimagesearch.com/2015/01/26/multi-scale-template-matching-using-python-opencv/). We are not going to go into that here but the link provided gives an example of how that could be implemented.  \n",
    "\n",
    "Let's display the portion of frame 420 where the Thymio is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T13:14:22.659420Z",
     "start_time": "2021-09-23T13:14:22.456052Z"
    }
   },
   "outputs": [],
   "source": [
    "template = video_imgs[420][312:484,446:611,:]\n",
    "plt.figure()\n",
    "plt.imshow(template)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now have a look at the opencv tutorial [here](https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_template_matching/py_template_matching.html) and apply the different template matching techniques on the image and the template. \n",
    "\n",
    "***Which methods seems the most robust and why?***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:39:46.408722Z",
     "start_time": "2020-05-05T15:39:45.202368Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have selected the algorithm that you believe to be the most appropriate, apply it to the set of images that were extracted from the video. To visualise the result of the process, we have provided a browse images function which when provided the individual images lets you nagivate through them with a slider. \n",
    "\n",
    "***What are the limitations of template matching?***\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T13:14:22.708767Z",
     "start_time": "2021-09-23T13:14:22.662504Z"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def browse_images(images, titles = None):\n",
    "    if titles == None:\n",
    "        titles = [i for i in range(len(images))]\n",
    "        \n",
    "    n = len(images)\n",
    "    def view_image(i):\n",
    "        plt.imshow(images[i], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        plt.title(titles[i], y=-0.5)\n",
    "        plt.show()\n",
    "    interact(view_image, i=(0,n-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:40:17.624471Z",
     "start_time": "2020-05-05T15:39:46.410795Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**/!\\ If you have problems with the slider widget, make sure you have installed and activated the jupyter-js-widgets/extension (see https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/install.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:40:21.028148Z",
     "start_time": "2020-05-05T15:40:20.792305Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
